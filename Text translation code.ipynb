{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8cf05d1-4c97-4a7e-ace7-97c36d5cd945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcd915f-d644-4b6f-8623-e41bd42223f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv(\"./Hindi_English_Truncated_Corpus.csv\")\n",
    "english_sentences = data[\"english_sentence\"]\n",
    "hindi_sentences = data[\"hindi_sentence\"]\n",
    "\n",
    "num_words = 10000\n",
    "oov_token = '<UNK>'\n",
    "english_vocab_size = num_words + 1\n",
    "hindi_vocab_size = num_words + 1\n",
    "MAX_WORDS_IN_A_SENTENCE = 16\n",
    "test_ratio = 0.2\n",
    "BATCH_SIZE = 512\n",
    "embedding_dim = 64\n",
    "hidden_units = 1024\n",
    "learning_rate = 0.006\n",
    "epochs = 100\n",
    "\n",
    "def preprocess_sentence(sen, is_english):\n",
    "\tif (type(sen) != str):\n",
    "\t\treturn ''\n",
    "\tsen = sen.strip('.')\n",
    "\t\n",
    "\t# insert space between words and punctuations\n",
    "\tsen = re.sub(r\"([?.!,¿;।])\", r\" \\1 \", sen)\n",
    "\tsen = re.sub(r'[\" \"]+', \" \", sen)\n",
    "\t\n",
    "\t# For english, replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\", \"'\")\n",
    "\tif(is_english == True):\n",
    "\t\tsen = re.sub(r\"[^a-zA-Z?.!,¿']+\", \" \", sen)\n",
    "\t\tsen = sen.lower()\n",
    "\t\n",
    "\tsen = sen.strip()\n",
    "\tsen = 'sentencestart ' + sen + ' sentenceend'\n",
    "\t\n",
    "\tsen = ' '.join(sen.split())\n",
    "\treturn sen\n",
    "\t\n",
    "# Loop through each datapoint having english and hindi sentence\n",
    "processed_e_sentences = []\n",
    "processed_h_sentences = []\n",
    "for (e_sen, h_sen) in zip(english_sentences, hindi_sentences):\n",
    "\tprocessed_e_sen = preprocess_sentence(e_sen, True)\n",
    "\tprocessed_h_sen = preprocess_sentence(h_sen, False)\n",
    "\tif(processed_e_sen == '' or processed_h_sen == '' or processed_e_sen.count(' ') >  (MAX_WORDS_IN_A_SENTENCE-1) or processed_h_sen.count(' ') > (MAX_WORDS_IN_A_SENTENCE-1)):\n",
    "\t\tcontinue\n",
    "\t\n",
    "\tprocessed_e_sentences.append(processed_e_sen)\n",
    "\tprocessed_h_sentences.append(processed_h_sen)\n",
    "\n",
    "print(\"Sentence examples: \")\n",
    "print(processed_e_sentences[0])\n",
    "print(processed_h_sentences[0])\n",
    "print(\"Length of English processed sentences: \" + str(len(processed_e_sentences)))\n",
    "print(\"Length of Hindi processed sentences: \" + str(len(processed_h_sentences)))\n",
    "\n",
    "def tokenize_sentences(processed_sentences, num_words, oov_token):\n",
    "\ttokenizer = Tokenizer(num_words = num_words, oov_token = oov_token)\n",
    "\ttokenizer.fit_on_texts(processed_sentences)\n",
    "\tword_index = tokenizer.word_index\n",
    "\tsequences = tokenizer.texts_to_sequences(processed_sentences)\n",
    "\tsequences = pad_sequences(sequences, padding = 'post')\n",
    "\treturn word_index, sequences, tokenizer\n",
    "\n",
    "english_word_index, english_sequences, english_tokenizer = tokenize_sentences(processed_e_sentences, num_words, oov_token)\n",
    "hindi_word_index, hindi_sequences, hindi_tokenizer = tokenize_sentences(processed_h_sentences, num_words, oov_token)\n",
    "\n",
    "# split into traning and validation set\n",
    "english_train_sequences, english_val_sequences, hindi_train_sequences, hindi_val_sequences = train_test_split(english_sequences, hindi_sequences, test_size = test_ratio)\n",
    "BUFFER_SIZE = len(english_train_sequences)\n",
    "\n",
    "# Batching the training set\n",
    "dataset = tf.data.Dataset.from_tensor_slices((english_train_sequences, hindi_train_sequences)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder = True)\n",
    "print(\"No. of batches: \" + str(len(list(dataset.as_numpy_iterator()))))\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "\t\n",
    "\tdef __init__(self, english_vocab_size, embedding_dim, hidden_units):\n",
    "\t\tsuper(Encoder, self).__init__()\n",
    "\t\tself.embedding = tf.keras.layers.Embedding(english_vocab_size, embedding_dim)\n",
    "\t\tself.gru = tf.keras.layers.GRU(hidden_units, return_sequences = True, return_state = True)\n",
    "\t\t\n",
    "\tdef call(self, input_sequence):\n",
    "\t\tx = self.embedding(input_sequence)\n",
    "\t\tencoder_sequence_output, final_encoder_state = self.gru(x)\n",
    "\t\t#\tDimensions of encoder_sequence_output => (BATCH_SIZE, MAX_WORDS_IN_A_SENTENCE, hidden_units)\n",
    "\t\t#\tDimensions of final_encoder_state => (BATCH_SIZE, hidden_units)\n",
    "\t\treturn encoder_sequence_output, final_encoder_state\n",
    "\n",
    "# initialize our encoder\n",
    "encoder = Encoder(english_vocab_size, embedding_dim, hidden_units)\n",
    "\n",
    "class BasicDotProductAttention(tf.keras.layers.Layer):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(BasicDotProductAttention, self).__init__()\n",
    "\t\t\n",
    "\tdef call(self, decoder_hidden_state, encoder_outputs):\n",
    "\t\t#\tDimensions of decoder_hidden_state => (BATCH_SIZE, hidden_units)\n",
    "\t\t#\tDimensions of encoder_outputs => (BATCH_SIZE, MAX_WORDS_IN_A_SENTENCE, hidden_units)\n",
    "\n",
    "\t\tdecoder_hidden_state_with_time_axis = tf.expand_dims(decoder_hidden_state, 2)\n",
    "\t\t#\tDimensions of decoder_hidden_state_with_time_axis => (BATCH_SIZE, hidden_units, 1)\n",
    "\t\tattention_scores = tf.matmul(encoder_outputs, decoder_hidden_state_with_time_axis)\n",
    "\t\t#\tDimensions of attention_scores => (BATCH_SIZE, MAX_WORDS_IN_A_SENTENCE, 1)\n",
    "\t\tattention_scores = tf.nn.softmax(attention_scores, axis = 1)\n",
    "\t\tweighted_sum_of_encoder_outputs = tf.reduce_sum(encoder_outputs * attention_scores, axis = 1)\n",
    "\t\t#\tDimensions of weighted_sum_of_encoder_outputs => (BATCH_SIZE, hidden_units)\n",
    "\n",
    "\t\treturn weighted_sum_of_encoder_outputs, attention_scores\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "\tdef __init__(self, hindi_vocab_size, embedding_dim, hidden_units):\n",
    "\t\tsuper(Decoder, self).__init__()\n",
    "\t\tself.embedding = tf.keras.layers.Embedding(hindi_vocab_size, embedding_dim)\n",
    "\t\tself.gru = tf.keras.layers.GRU(hidden_units, return_state = True)\n",
    "\t\tself.word_probability_layer = tf.keras.layers.Dense(hindi_vocab_size, activation = 'softmax')\n",
    "\t\tself.attention_layer = BasicDotProductAttention()\n",
    "\t\t\n",
    "\tdef call(self, decoder_input, decoder_hidden, encoder_sequence_output):\n",
    "\t\t\n",
    "\t\tx = self.embedding(decoder_input)\n",
    "\t\t#\tDimensions of x => (BATCH_SIZE, embedding_dim)\n",
    "\t\tweighted_sum_of_encoder_outputs, attention_scores = self.attention_layer(decoder_hidden, encoder_sequence_output)\n",
    "\t\t#\tDimensions of weighted_sum_of_encoder_outputs => (BATCH_SIZE, hidden_units)\n",
    "\t\tx = tf.concat([weighted_sum_of_encoder_outputs, x], axis = -1)\n",
    "\t\tx = tf.expand_dims(x, 1)\n",
    "\t\t#\tDimensions of x => (BATCH_SIZE, 1, hidden_units + embedding_dim)\n",
    "\t\tdecoder_output, decoder_state = self.gru(x)\n",
    "\t\t#\tDimensions of decoder_output => (BATCH_SIZE, hidden_units)\n",
    "\t\tword_probability = self.word_probability_layer(decoder_output)\n",
    "\t\t#\tDimensions of word_probability => (BATCH_SIZE, hindi_vocab_size)\n",
    "\t\treturn word_probability, decoder_state, attention_scores\n",
    "\n",
    "# initialize our decoder\n",
    "decoder = Decoder(hindi_vocab_size, embedding_dim, hidden_units)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction='none')\n",
    "def loss_function(actual_words, predicted_words_probability):\n",
    "\tloss = loss_object(actual_words, predicted_words_probability)\n",
    "\tmask = tf.where(actual_words > 0, 1.0, 0.0)\n",
    "\treturn tf.reduce_mean(mask * loss)\n",
    "\n",
    "def train_step(english_sequences, hindi_sequences):\n",
    "\tloss = 0\n",
    "\twith tf.GradientTape() as tape:\n",
    "\t\tencoder_sequence_output, encoder_hidden = encoder(english_sequences)\n",
    "\t\tdecoder_hidden = encoder_hidden\n",
    "\t\tdecoder_input = hindi_sequences[:, 0]\n",
    "\t\tfor i in range(1, hindi_sequences.shape[1]):\n",
    "\t\t\tpredicted_words_probability, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_sequence_output)\n",
    "\t\t\tactual_words = hindi_sequences[:, i]\n",
    "\t\t\t# if all the sentences in batch are completed\n",
    "\t\t\tif np.count_nonzero(actual_words) == 0:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tloss += loss_function(actual_words, predicted_words_probability)\n",
    "\n",
    "\t\t\tdecoder_input = actual_words\n",
    "\n",
    "\tvariables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\tgradients = tape.gradient(loss, variables)\n",
    "\toptimizer.apply_gradients(zip(gradients, variables))\n",
    "\treturn loss.numpy()\n",
    "\n",
    "all_epoch_losses = []\n",
    "training_start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "\tepoch_loss = []\n",
    "\tstart_time = time.time()\n",
    "\tfor(batch, (english_sequences, hindi_sequences)) in enumerate(dataset):\n",
    "\t\tbatch_loss = train_step(english_sequences, hindi_sequences)\n",
    "\t\tepoch_loss.append(batch_loss)\n",
    "\n",
    "\tall_epoch_losses.append(sum(epoch_loss)/len(epoch_loss))\n",
    "\tprint(\"Epoch No.: \" + str(epoch) + \" Time: \" + str(time.time()-start_time))\n",
    "\n",
    "print(\"All Epoch Losses: \" + str(all_epoch_losses))\n",
    "print(\"Total time in training: \" + str(time.time() - training_start_time))\n",
    "\n",
    "plt.plot(all_epoch_losses)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Epoch Loss\")\n",
    "plt.show()\n",
    "\n",
    "def get_sentence_from_sequences(sequences, tokenizer):\n",
    "\treturn tokenizer.sequences_to_texts(sequences)\n",
    "\n",
    "# Testing\n",
    "def translate_sentence(sentence):\n",
    "\tsentence = preprocess_sentence(sentence, True)\n",
    "\tsequence = english_tokenizer.texts_to_sequences([sentence])[0]\n",
    "\tsequence = tf.keras.preprocessing.sequence.pad_sequences([sequence], maxlen = MAX_WORDS_IN_A_SENTENCE, padding = 'post')\n",
    "\tencoder_input = tf.convert_to_tensor(sequence)\n",
    "\tencoder_sequence_output, encoder_hidden = encoder(encoder_input)\n",
    "\tdecoder_input = tf.convert_to_tensor([hindi_word_index['sentencestart']])\n",
    "\tdecoder_hidden = encoder_hidden\n",
    "\t\n",
    "\tsentence_end_word_id = hindi_word_index['sentenceend']\n",
    "\thindi_sequence = []\n",
    "\tfor i in range(MAX_WORDS_IN_A_SENTENCE*2):\n",
    "\t\tpredicted_words_probability, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_sequence_output)\n",
    "\t\t# taking the word with maximum probability\n",
    "\t\tpredicted_word_id = tf.argmax(predicted_words_probability[0]).numpy()\n",
    "\t\thindi_sequence.append(predicted_word_id)\n",
    "\t\t# if the word 'sentenceend' is predicted, exit the loop\n",
    "\t\tif predicted_word_id == sentence_end_word_id:\n",
    "\t\t\tbreak\n",
    "\t\tdecoder_input = tf.convert_to_tensor([predicted_word_id])\n",
    "\tprint(sentence)\n",
    "\treturn get_sentence_from_sequences([hindi_sequence], hindi_tokenizer)\n",
    "\n",
    "# print translated sentence\n",
    "print(translate_sentence(\"How are you\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedb527d-e3b5-4fda-99d3-2265ef0dfa7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
